# Chapter 4: Discussion and Implications

## Introduction

This chapter interprets the findings presented in Chapter 3, situating them within the broader context of existing literature on trust, human-AI interaction, and career coaching. The discussion addresses each research question in turn, drawing connections between participants' narratives and theoretical frameworks from the literature review. Subsequently, practical implications are derived for the design and implementation of hybrid AI-human coaching models, with specific recommendations for ODA's service development. The chapter concludes with an acknowledgment of the study's limitations and suggestions for future research directions.

---

## 4.1 Interpretation of Key Findings

### 4.1.1 Understanding Trust Dynamics in AI versus Human Coaching

The finding that participants articulated fundamentally different trust architectures for human and AI coaching—relational trust built through presence versus functional trust derived from absence—aligns with and extends existing literature on trust in human-AI interaction.

**The Persistence of the Human Trust Advantage.** The prevalent trust gap favoring human coaches (mean 6.3/10 versus 5.7/10 for AI) echoes broader findings in the literature on trust in automation and AI systems. Yokoi et al. (2021) observed similar patterns in medical contexts, where patients preferred human physicians over AI diagnostic systems despite comparable accuracy, citing the importance of empathy and the "human touch" in sensitive decision domains. Career coaching, like healthcare, involves personal vulnerability and consequential decisions—contexts where humans appear to retain a trust advantage rooted in evolutionary and social factors.

Several mechanisms may explain this persistent gap. First, trust is fundamentally a social construct that evolved in the context of human-to-human interaction. The ability to read facial expressions, perceive emotional states, and engage in reciprocal vulnerability creates conditions for trust formation that AI cannot replicate, regardless of its functional capabilities. Second, participants' distrust of AI often stemmed from opacity—not understanding how AI generates its recommendations. This aligns with research on explainable AI (XAI), which suggests that transparency about decision-making processes is crucial for user trust (Arrieta et al., 2020). Third, the novelty factor may contribute: participants have lifetime experience calibrating trust in humans but limited experience with AI coaching specifically, leading to uncertainty that manifests as caution.

**The Distinct Value of AI's "Absence-Based" Safety.** However, the finding that AI trust derives from absence—of judgment, fatigue, ego, and social memory—represents an important nuance that complicates simple narratives of human superiority. For specific populations and use cases, this absence-based safety provides genuine value that human coaching cannot easily replicate.

The cultural and immigrant safety finding is particularly significant. Participants from backgrounds emphasizing pride and capability—those facing "double performance pressure"—found AI's judgment-free quality liberating. As Amir articulated, "I come from a culture where you have pride, you do not show weakness too much. And also as an immigrant, you always want to appear capable. The AI is like, okay, you can be messy here." This suggests that AI coaching may offer particular value for populations who face additional barriers to vulnerability in human interactions, potentially democratizing access to a form of psychological safety that might otherwise be inaccessible.

This finding connects to literature on help-seeking barriers, which documents that stigma, performance expectations, and identity concerns can prevent individuals from seeking support they need (Vogel et al., 2007). AI's inability to judge may lower these barriers for specific populations, suggesting a complementary role rather than simple inferiority.

**Trust Rupture and Recovery Asymmetries.** The finding that human trust is fragile—building gradually but collapsing instantly—while AI trust is more stable has significant implications. Tobias's account of somatic trust rupture ("My stomach dropped... I made myself vulnerable, and he slapped it away") reveals that relational trust operates at a pre-cognitive, embodied level. Once violated, such trust proves resistant to repair, explaining why Tobias could not engage with objectively useful advice from a coach who had dismissed him.

This asymmetry suggests different risk profiles for each modality. Human coaching offers higher potential trust depth but carries greater risk of lasting damage from missteps. AI coaching offers more consistent (if bounded) safety but limited potential for the deep trust that enables transformative work. These risk-reward profiles have implications for service design: human coaches must be carefully selected and trained to avoid trust-destructive behaviors, while AI systems can be more consistently deployed but should not be positioned for trust-intensive work.

### 4.1.2 Understanding Effectiveness Perceptions

**Human Effectiveness Through Relational Mechanisms.** The finding that human coaches were perceived as more effective for emotional support, contextual knowledge, accountability, and high-stakes decisions aligns with established coaching theory. Effective coaching relationships are characterized by active listening, empathy, and encouragement—relational processes that enable clients to access their own resources and move toward action (Ives, 2008). Participants' accounts of effective human coaching consistently referenced these relational elements: feeling understood, receiving personalized challenge, and experiencing accountability through relationship.

The accountability finding is particularly noteworthy. Katharina's statement—"I do not feel accountable to AI. I can ignore notifications"—points to a fundamental limitation in AI's effectiveness for behavior change. Accountability requires relational weight; the social consequences of disappointing another person create commitment that algorithmic reminders cannot replicate. This connects to research on social facilitation and commitment devices, which documents that interpersonal obligation is a powerful motivator for behavior change (Rogers et al., 2015).

**AI Effectiveness Through Cognitive Mechanisms.** Conversely, AI's perceived effectiveness for structuring, iterating, and overcoming paralysis operates through cognitive rather than relational mechanisms. The "blank page problem" that Simon described—and AI's capacity to provide "something to react to"—represents a genuine cognitive contribution. AI reduces the executive function burden of initiation, enabling participants to engage with revision rather than creation from nothing.

Laura #2's account of AI-driven career decision-making illustrates AI's potential for structured reflection. Her three-week process—involving frameworks, criteria, scenarios, and ultimately a decision memo—produced what she described as "a real coaching moment" when AI asked, "what are you protecting by not deciding?" This suggests that well-designed AI interactions can prompt genuine insight, not merely provide information. However, Laura #2 also noted that she still sought human validation, suggesting AI's insights require human confirmation for full impact.

**Complementarity Over Competition.** The pervasive theme of complementarity—participants viewing AI and human coaching as suited for different tasks rather than competing for the same functions—has important implications. Rather than asking "which is better?", participants naturally asked "for which tasks is each better?"

This finding aligns with research on human-AI collaboration, which increasingly emphasizes task-appropriate division of labor over wholesale replacement (Dellermann et al., 2019). The Conference Board's findings that AI coaching lacks "personal connection and spontaneity" while excelling at "data processing and routine guidance" mirror participants' perceptions precisely. The implication is that optimal coaching models should match modality to task rather than attempt universal application of either.

### 4.1.3 The Trust-Effectiveness Nexus

**The Gateway Mechanism.** The finding that trust functions as a gateway controlling disclosure depth represents perhaps the most significant theoretical contribution of this study. The causal chain—Trust → Disclosure Depth → Advice Quality → Action Willingness—explains *how* trust produces effectiveness, not merely *that* they correlate.

This mechanism has precedent in adjacent literatures. In leadership research, trust in a leader improves perceived leader effectiveness through similar mechanisms: trusted leaders receive more candid information and face less resistance to directives (Dirks & Ferrin, 2002). In therapeutic contexts, the working alliance—comprising trust, agreement on goals, and collaboration—is among the strongest predictors of therapeutic outcomes (Horvath & Symonds, 1991). The coaching literature similarly emphasizes that trust and rapport are prerequisites to effective coaching, enabling the honest self-examination that produces insight and change (de Haan et al., 2013).

The "performance problem"—where low trust causes participants to share polished rather than authentic versions of their situations—illuminates why some coaching fails despite skilled coaches. If the coach works with false information, even excellent technique cannot address the real barriers. This has practical implications: building trust is not merely a nice-to-have but a functional prerequisite for effective coaching.

**Positive Feedback Loops.** Participants' accounts also revealed that the trust-effectiveness relationship operates bidirectionally. Demonstrated effectiveness can build trust: several participants described giving AI "a chance" after initial skepticism and developing greater confidence through positive experiences. This suggests positive feedback loops: initial trust enables engagement, which produces (if the system is functional) positive experiences, which reinforce trust, which enables deeper engagement.

For AI systems specifically, this implies that early interactions are crucial. If AI can quickly demonstrate value—proving useful and credible in initial encounters—users may enter a positive cycle that builds trust over time. Conversely, early failures may trigger negative spirals where distrust prevents the engagement that might build confidence.

**Task Type as Moderator.** The finding that task type moderates the trust-effectiveness relationship adds important nuance. For emotional and identity-related work, trust remains essential—the gateway mechanism dominates. For bounded cognitive tasks, effectiveness can occur with partial trust through verification. Silvia's articulation of this distinction—"Trust is like a gate for humans. For AI, trust is like a filter, not a gate"—captures the moderation elegantly.

This moderation explains the quantitative finding that 58% of participants rated AI effectiveness higher than AI trust. For bounded tasks, verification can substitute for faith: participants extract value while maintaining skepticism, checking outputs rather than accepting them on trust. This pragmatic engagement pattern suggests that AI systems need not achieve full trust to be useful—they can operate effectively in a "verify then apply" mode for appropriate tasks.

---

## 4.2 Answering the Research Questions

### 4.2.1 RQ1: How do early-career individuals in Germany describe and compare their trust in AI-based career coaching tools and human career coaching consultations?

Participants described trust in human coaching as a multi-component architecture requiring demonstrated competence, explicit confidentiality, personalization ("being seen"), and the capacity to challenge without shaming. This trust is relational—built through consistent behavior over time—but fragile, capable of instant collapse through dismissiveness or judgment, with rupture often proving permanent. Trust in AI derived from fundamentally different sources: the absence of judgment, fatigue, ego, and social memory, creating a "judgment-free zone" particularly valuable for those facing cultural or identity-based performance pressure. However, AI trust was bounded by concerns about overconfidence, hallucinations, lack of contextual understanding, and privacy, leading to self-censoring that limited disclosure depth. Comparatively, participants generally trusted human coaches more (mean 6.3/10 versus 5.7/10 for AI), though substantial individual variation existed based on prior experiences with each modality.

### 4.2.2 RQ2: How do these individuals describe and compare the perceived effectiveness of AI-based career coaching tools and human career coaching consultations for their career-related needs?

Participants perceived human coaching as more effective for tasks requiring contextual knowledge, emotional support, accountability, and high-stakes decision-making—domains where relational presence provides irreplaceable value. AI was perceived as more effective for cognitive and structural tasks: overcoming blank-page paralysis, generating options, iterating drafts, and structuring chaos into actionable steps. Rather than viewing these modalities as competing, participants viewed them as complementary, actively matching each to appropriate tasks. Quantitatively, AI effectiveness ratings (mean 6.5/10) slightly exceeded human effectiveness ratings (mean 5.6/10), though human effectiveness showed higher variance—exceptional when the match was right, but poor when it wasn't—while AI provided more consistent but bounded value.

### 4.2.3 RQ3: In participants' narratives, how are trust and perceived effectiveness connected, and how do they influence preferences for AI, human, or hybrid coaching models?

Trust and perceived effectiveness were linked through a gateway mechanism: trust enables disclosure depth, which determines advice quality, which affects action willingness, ultimately producing (or failing to produce) effective outcomes. This mechanism was moderated by task type—essential for emotional and identity work, less critical for bounded cognitive tasks where verification can substitute for trust. The mechanism also admitted exceptions: trusted sources sometimes lacked structure (trusted but not effective), while distrusted sources could provide useful outputs at cognitive cost (effective but not trusted). These dynamics produced a universal preference for hybrid models that sequence AI and human support strategically: AI for preparation and follow-up (bounded cognitive tasks with lower trust requirements), humans for pivotal moments (high-stakes decisions requiring trust-enabled depth). A particularly significant finding was that AI preparation enables better human conversations by reducing initial "messiness" and focusing expensive human time on high-value decisions.

---

## 4.3 Implications for Theory and Practice

### 4.3.1 Theoretical Implications

This study contributes to several theoretical domains. First, it provides qualitative evidence extending models of technology acceptance to coaching contexts, suggesting that trust and perceived usefulness operate through distinct mechanisms for relational (human) versus functional (AI) systems. The trust-as-gateway finding offers a mechanism-level explanation for why trust matters, complementing outcome-focused research with process-level understanding.

Second, the study contributes to human-AI collaboration literature by documenting how users naturally develop task-appropriate mental models for dividing work between modalities. Participants did not require instruction to conceptualize complementary roles; they intuitively developed theories of comparative advantage and applied them in their usage patterns. This suggests that hybrid system design may benefit from surfacing and supporting these intuitive models rather than imposing designer-defined divisions.

Third, the cultural and immigrant safety finding suggests that AI coaching may offer particular value for populations facing identity-based barriers to help-seeking—a perspective underrepresented in current literature that tends to position AI as simply "less effective" than human alternatives. For specific populations and use cases, AI's absence-based safety may represent a genuine accessibility advantage rather than a mere convenience.

### 4.3.2 Practical Implications for ODA's Hybrid AI-Human Coaching Model

Based on participants' expressed needs, preferences, and concerns, the following recommendations address how ODA can design and implement a hybrid coaching service that earns user trust and delivers perceived effectiveness.

#### Building User Trust in the AI Component

**Recommendation 1: Design for transparency and explainability.** Participants' distrust of AI stemmed partly from opacity—not understanding how it generates recommendations. ODA's AI should explain its suggestions, cite sources where applicable, and acknowledge uncertainty when present. Rather than presenting outputs as authoritative, the system should frame them as starting points for reflection ("Here are some possibilities to consider based on..."). This transparency aligns with participants' preferences for coaches who acknowledge uncertainty rather than projecting false confidence.

**Recommendation 2: Establish human-mediated introduction.** Several participants indicated that trust could transfer from humans to systems. ODA could leverage this by having human coaches introduce the AI component to new clients, explaining its role, capabilities, and limitations. When a trusted human vouches for a system, initial skepticism may decrease. The human coach might say: "Between our sessions, you'll work with our AI assistant, which I've found really helpful for [specific tasks]. It's not meant to replace our conversations, but to help you prepare and follow through."

**Recommendation 3: Ensure empathetic, non-robotic interaction design.** Participants disliked scripted, mechanical-feeling interactions. ODA's AI should employ natural language patterns, acknowledge emotional content appropriately, and avoid the "template advice" that participants associated with low competence. The system should feel supportive without claiming emotions it cannot have—containment without pretending to companionship. Phrases like "That sounds challenging" or "It makes sense you'd feel uncertain" can acknowledge difficulty without false claims of empathy.

**Recommendation 4: Address privacy concerns proactively.** Self-censoring due to privacy concerns limited participants' disclosure to AI. ODA should clearly communicate data handling practices, offer options for limiting data retention, and reassure users about confidentiality. Consider allowing users to delete specific conversations or to interact in a "private mode" that doesn't retain content. Upfront transparency about privacy may enable deeper engagement.

**Recommendation 5: Enable human escalation.** Participants wanted assurance that human support was available when needed. ODA's AI should include clear pathways to human escalation—not buried in menus but readily accessible. When the AI detects complex emotional content or high-stakes decisions, it might proactively suggest: "This seems like something worth discussing with your coach. Would you like me to note this for your next session?" This safety net may increase willingness to engage with AI initially.

#### Enhancing Perceived Effectiveness

**Recommendation 6: Establish clear role division aligned with comparative advantage.** ODA should design the hybrid model with explicit role division matching participants' mental models: AI for structure, iteration, preparation, and follow-up; humans for context, emotional processing, accountability, and pivotal decisions. This division should be communicated to users so expectations are calibrated appropriately.

| AI-Led Tasks | Human-Led Tasks |
|--------------|-----------------|
| Initial intake and situation mapping | Values clarification and identity work |
| Option generation and breadth exploration | Reality-checking options against context |
| CV/application drafting and iteration | Interview practice with nuanced feedback |
| Scheduling and progress tracking | Accountability and commitment renewal |
| Between-session exercises and reflection prompts | High-stakes decision support |
| Information gathering and synthesis | Emotional processing after setbacks |

**Recommendation 7: Use AI to enhance human sessions.** Participants valued AI's ability to structure thinking before human conversations. ODA should design AI interactions to prepare clients for coaching sessions: summarizing progress, identifying questions, and organizing thoughts. The AI might generate a pre-session brief for both client and coach: "Since your last session, you've worked on X, expressed concern about Y, and identified Z as a priority." This makes expensive human time more productive.

**Recommendation 8: Provide AI-supported continuity between sessions.** Coaching effectiveness often depends on between-session work that can lapse without support. AI can maintain momentum through check-ins, reminders, and accountability prompts between human sessions. However, these should feel supportive rather than nagging—participants noted they could ignore AI reminders without consequence. Consider designing prompts that feel like collaborative check-ins rather than notifications.

**Recommendation 9: Enable quick wins to demonstrate value.** Given the positive feedback loop between effectiveness and trust, AI should be designed to deliver early, tangible value. Quick wins—successful CV improvements, useful option generation, helpful interview preparation—build confidence that encourages deeper engagement. Consider designing onboarding to include low-stakes tasks where AI can demonstrate competence before users are asked to engage on higher-stakes issues.

#### Hybrid Interaction Models

**Recommendation 10: Implement sequential hybrid workflow.** Based on participants' preferences, ODA should implement a sequential model: AI → Human → AI → Human, with AI handling preparation and follow-up while humans address pivotal moments. A typical engagement might flow:

1. **Week 1 (AI):** Intake, situation mapping, values exploration, option generation
2. **Week 2 (Human):** Deep discussion of direction, reality-checking, initial commitments
3. **Weeks 3-4 (AI):** Action planning, draft creation, progress tracking, reflection exercises
4. **Week 5 (Human):** Review of progress, addressing obstacles, accountability renewal
5. **Ongoing (AI + periodic Human):** Maintenance, with human sessions for significant transitions

**Recommendation 11: Allow user-driven sequencing flexibility.** While the sequential model suits most participants, some needed human safety before engaging with AI (Katharina: "At the beginning, I needed relational safety to admit what I was avoiding"). ODA should allow users to begin with human sessions if preferred, with AI introduced after trust is established. User preferences should drive sequencing rather than forcing a uniform model.

**Recommendation 12: Maintain seamless handoffs.** When users move between AI and human touchpoints, continuity matters. The human coach should reference AI interactions ("I saw you worked through some career options this week"), and AI should reference coaching sessions ("In your last session with [Coach], you discussed..."). This integration signals that both components are part of a unified service rather than disconnected tools.

#### Communication and Trust-Building

**Recommendation 13: Frame the hybrid value proposition clearly.** ODA's marketing and onboarding should articulate the hybrid model's value in terms that resonate with user priorities: "Get immediate, structured support from our AI tools, plus the insight and accountability of a dedicated human coach who knows your story." Address trust concerns directly: "Our AI helps you prepare and reflect, while your coach provides the nuanced guidance that only human understanding can offer."

**Recommendation 14: Set appropriate expectations.** Participants' trust was damaged when AI claimed or implied capabilities it lacked. ODA should honestly communicate limitations: the AI is excellent for structuring and exploring but should not be treated as a source of facts without verification, and it cannot replace human judgment for consequential decisions. Managing expectations prevents the disappointment that erodes trust.

**Recommendation 15: Train human coaches to integrate with AI.** Coaches should be trained to acknowledge and build upon AI interactions rather than treating them as separate or competitive. Coaches might review AI session summaries before human meetings, reference insights that emerged in AI conversations, and assign AI-supported exercises between sessions. This positions human and AI as a team rather than alternatives.

**Recommendation 16: Collect and respond to feedback continuously.** Trust and effectiveness perceptions will evolve as users gain experience with the hybrid model. ODA should implement ongoing feedback mechanisms—brief post-session ratings, periodic deeper check-ins—to identify trust barriers or effectiveness gaps. This feedback should drive iterative improvement, demonstrating that user concerns are heard and addressed.

---

## 4.4 Limitations

Several limitations should be acknowledged when interpreting this study's findings.

**Sample Size and Generalizability.** The study included 19 participants, a sample appropriate for qualitative exploration but insufficient for statistical generalization. Findings represent patterns within this specific group rather than population-level truths. The sample was limited to early-career individuals in Germany; different age groups, career stages, cultural contexts, or countries might yield different perceptions of AI versus human coaching.

**Sample Characteristics.** While the sample included diverse nationalities and professional backgrounds, all participants were educated professionals in urban German settings. Individuals with lower educational attainment, different career types, or rural locations might have different relationships with technology and coaching. The sample's relative comfort with technology may produce more favorable AI perceptions than would be found in less tech-savvy populations.

**Breadth of "AI Coaching" Concept.** The study treated "AI-based career coaching tools" as a category, but participants' experiences and mental images varied—some referenced specific tools (ChatGPT), others described hypothetical AI coaches. This heterogeneity means findings reflect perceptions of AI coaching in general rather than evaluation of any specific system. Participants' views were sometimes based on extrapolation from limited experience rather than extensive use, introducing potential divergence between perceived and actual effectiveness.

**Retrospective Self-Report.** Qualitative interviews rely on participants' retrospective accounts and self-perceptions. Memory is reconstructive, and participants may have rationalized, simplified, or narratively organized their experiences in ways that differ from what actually occurred. Their stated preferences may also differ from their actual behavior when faced with real choices.

**Researcher Influence.** As a single researcher conducted all interviews and analysis, the findings inevitably reflect interpretive choices about which themes to emphasize and how to frame results. While systematic procedures were followed and themes were grounded in participant language, qualitative research necessarily involves researcher subjectivity. Different researchers might emphasize different aspects of the same data.

**Cross-Sectional Design.** The study captured perceptions at a single point in time. Trust and effectiveness perceptions likely evolve with experience—a participant skeptical of AI today might become more trusting after positive experiences, or vice versa. Longitudinal research would be needed to understand these dynamics.

**Hypothetical Elements.** Some participants had limited or no experience with AI coaching specifically, leading them to answer based on expectations rather than experience. While their anticipated perceptions are valuable data, they may not predict actual responses to real AI coaching interactions.

---

## 4.5 Future Research

Based on the findings and limitations of this study, several directions for future research are suggested.

**Larger and More Diverse Samples.** Quantitative research with larger, more diverse samples could assess the generalizability of the patterns identified here. Survey research could measure the prevalence of specific trust barriers, effectiveness perceptions, and hybrid preferences across demographic groups, professional fields, and cultural contexts.

**Longitudinal and Experimental Designs.** Experimental research could assign participants to different coaching conditions (AI-only, human-only, various hybrid configurations) and measure trust and effectiveness outcomes over time. This would address whether expressed preferences align with actual experiences and whether trust evolves with exposure. Longitudinal designs could track how trust-effectiveness relationships develop and stabilize.

**Evaluation of Specific Hybrid Implementations.** Once hybrid services like ODA's are implemented, research should directly evaluate user experiences. Do the design recommendations proposed here actually enhance trust and effectiveness? What barriers emerge in practice that were not anticipated? Iterative research accompanying service development would refine understanding of what works.

**Mechanisms of Trust Transfer.** The finding that human introduction might transfer trust to AI systems warrants further investigation. Under what conditions does trust transfer occur? What characteristics of the human vouching for AI matter? Can AI-generated trust transfer to affiliated humans? Understanding these dynamics could inform hybrid service design.

**Cultural and Individual Differences.** The cultural safety finding for immigrants suggests that AI coaching may be particularly valuable for specific populations. Research should explore which individual differences (personality, technology attitudes, cultural background, prior experiences) predict openness to AI coaching, and whether tailored approaches could serve different user segments.

**AI Design for Trust and Effectiveness.** Interdisciplinary research combining human-computer interaction, psychology, and coaching science could explore how AI systems should be designed to maximize trust and effectiveness. What interaction patterns, communication styles, and capability boundaries optimize user experience? How can AI systems embody transparency and manage expectations without undermining confidence?

**Integration of Quantitative Outcome Measures.** This study relied on perceived effectiveness. Future research should incorporate objective outcome measures—career advancement, salary changes, goal attainment—to assess whether perceived effectiveness corresponds to actual results, and whether AI and human coaching produce different outcome profiles.

---

## Summary

This chapter has interpreted the study's findings in light of existing literature, answered the three research questions, and derived practical implications for ODA's hybrid AI-human coaching model. Trust in human and AI coaching operates through fundamentally different mechanisms—relational presence versus absence of judgment—with each offering distinct value for different populations and tasks. Effectiveness perceptions reveal complementarity rather than competition, with participants naturally matching modality to task type. Trust and effectiveness are linked through a gateway mechanism moderated by task type, producing a universal preference for hybrid models that sequence AI and human support strategically.

For ODA, these findings suggest designing AI components for transparency and empathy, establishing human-mediated introduction, maintaining clear role divisions aligned with comparative advantage, and enabling seamless integration between touchpoints. Success will depend on managing expectations, building trust through demonstrated value, and continuously refining the model based on user feedback. The limitations of this exploratory study point toward larger-scale, longitudinal, and experimental research to validate and extend these initial findings.
