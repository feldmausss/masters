# Chapter 3: Findings

## Introduction

This chapter presents the findings from 19 semi-structured interviews conducted with early-career individuals in Germany regarding their experiences with, and perceptions of, AI-based career coaching tools and human career coaching consultations. Through thematic analysis following the approach outlined by Braun and Clarke (2006), several key themes emerged regarding (a) trust in AI-based versus human coaching, (b) perceived effectiveness of these modalities, and (c) the interplay between trust and effectiveness that influences coaching preferences. The analysis reveals that trust and effectiveness operate through distinct yet interconnected mechanisms, with participants demonstrating nuanced, task-specific preferences rather than wholesale endorsement or rejection of either modality.

Pseudonyms are used throughout when quoting participants. All quotations are drawn directly from interview transcripts (see Appendix A for complete transcripts). The findings are organized according to the three research questions, with thematic sub-sections that emerged from the data analysis.

---

## 3.1 Trust in Human versus AI-Based Career Coaching (RQ1)

The first research question asked: *How do early-career individuals in Germany describe and compare their trust in AI-based career coaching tools and human career coaching consultations?* The analysis revealed that trust operates through fundamentally different mechanisms for human and AI coaching, with participants articulating distinct trust architectures, drivers, and barriers for each modality.

### 3.1.1 Trust in Human Coaches: The Multi-Component Architecture

Participants consistently described trust in human coaching not as a single attitude but as a multi-component architecture requiring several conditions to be simultaneously present. Four primary pillars emerged across interviews: demonstrated competence, explicit confidentiality, the experience of being "seen," and the capacity to challenge without shaming.

**Competence as Foundation.** Domain-specific knowledge emerged as the foundational requirement for trust. Participants distinguished sharply between generic advice and contextually grounded guidance. As Amir explained:

> "Competence, like understanding UX research and the job market, not only general advice. And also, confidentiality... And the vibe. Like, if I feel judged, then no. I need them to ask good questions, not only give advice. If they only give advice, then it is like, okay, you are just a blog post." (Amir)

This comparison to "a blog post" captures a sentiment expressed across multiple interviews: that competence is demonstrated through personalized engagement rather than generic prescriptions. Simon articulated similar criteria: "Competence... does the person actually know what they are talking about. The other is fit... do they understand my goals and constraints. I also value honesty about uncertainty."

Oleg provided an illuminating account of what competence looked like in practice with his mentor:

> "He did not share details with other people... He would say, do not mention my name when you talk to HR, just say you heard it from internal sources. He protected himself and me. That felt professional... And also competence. He knew the process. Like, he said, you will have HR screening, then technical interview, then business case, then final with hiring manager. He described what questions they ask. And it was super accurate." (Oleg)

This combination of discretion and procedural knowledge demonstrated competence that Oleg could verify—and did verify when he experienced the interview process exactly as predicted.

Mihir similarly emphasized that competence must be contextually relevant:

> "Competence, like they know German hiring, they know engineering roles. Empathy, but not pity. Understanding international constraints, like visa, language, working student. Confidentiality. Structure, like they have a process, not random talking. And also feeling seen, like they remember details, they connect patterns." (Mihir)

**Confidentiality and Explicit Boundaries.** Participants noted that trust formation accelerated when coaches explicitly stated confidentiality and boundaries, rather than leaving them implicit. Katharina's account was particularly illustrative:

> "I need confidentiality, obviously, and I need clear structure. And I said, I need challenge without shame... she repeated it back... challenge without shaming, got it. If at any point you feel shame in the room, tell me, because shame blocks learning. That was like, wow. That made me trust her quickly." (Katharina)

The coach's act of repetition—"challenge without shaming, got it"—functioned as a trust-forming contract, signaling that Katharina's needs had been heard and would be protected. Jonas similarly emphasized that confidentiality must be explicit: "He explicitly said, this stays between us, I will not talk to your manager. That matters. Because in a scale-up, gossip travels fast."

Alina described her experience with confidentiality in coaching:

> "Confidentiality. She explained that everything stays between us, and she had a professional vibe. Like, she did not push me to buy ten sessions. She was also empathetic, but not pity. She validated that boredom can be a sign that you outgrow a role. Also, she understood the bank culture a bit. Not deeply, but enough to know that compliance is sensitive and that internal moves are political. That made me feel she is not naive." (Alina)

**Being Seen Through Personalization.** The experience of being "seen" emerged as a distinct trust component, separate from competence. Participants valued coaches who remembered details across sessions, noticed behavioral patterns, and demonstrated genuine attention. Katharina enumerated this among her trust requirements: "Confidentiality, structure, competence in my market, empathy, challenge, consistency, and being seen. Like remembering details, catching patterns, not just listening passively."

Noura described what being taken seriously looked like in her positive mentor interaction:

> "She started with, tell me your story. And I explained, Ausbildung, HR coordinator, part-time studies. And I said I feel not credible. And she said, listen, this path is not a weakness. In Germany, Ausbildung is valued. And HR operations gives you understanding of processes and data quality, which is crucial for analytics. She made it sound like, you already have something relevant." (Noura)

The inverse—feeling like a case number—was cited as trust-destructive. Sofia described the contrast between her two mentors:

> "She [first mentor] asked about my team, the meeting culture, my language situation. And she was credible, like she had done the path. Also confidentiality. She explicitly said, what we talk about stays here. That made me relax... [Second mentor] felt judgemental, maybe not intentionally. Like she would say, why did you not do this already? And I was like, eehm, because I am scared? But I did not want to say that. Also she did not remember details. It felt transactional." (Sofia)

**Challenge Without Shame.** Perhaps the most nuanced trust component was participants' desire for productive challenge that did not tip into shaming or invalidation. This distinction appeared repeatedly across interviews, with participants valuing coaches who could confront avoidance or blind spots without making them feel judged. The phrase "challenge without shame" became a recurring formulation.

Slava articulated the desired balance:

> "I do not want someone who is too soft and just validates. I also do not want someone who is like a guru. I want practical. And I want someone who can handle complexity. And I want boundaries, like, what they can do, what they cannot do." (Slava)

### 3.1.2 Trust Rupture: The Asymmetry of Building and Breaking

A striking finding concerned the asymmetry between trust formation and trust destruction. While building trust required consistent behavior over multiple interactions, destroying it could occur in a single moment. Tobias provided the most vivid account of trust rupture:

> "My stomach dropped. Like, physically. I felt heat in my face. And my throat got tight. I remember I stopped breathing for a second. I made myself vulnerable, and he slapped it away. And then I became very careful." (Tobias)

The somatic nature of Tobias's response—stomach dropping, heat in face, throat tightening—reveals that trust operates at a whole-person level, not merely as cognitive assessment. The phrase "slapped it away" conveys the perceived violence of dismissal, and "became very careful" signals lasting damage that fundamentally altered the coaching relationship.

Tobias later elaborated on how the same message could land differently depending on delivery: "The coach said it like, shut up. The colleague said it like, you are not alone. Big difference." This comparison crystallizes how trust is built or broken not only by content but by manner—the same observation can construct or rupture safety depending on how it is framed.

Noura described a similarly damaging interaction with a LinkedIn mentor:

> "He was like Head of People Analytics something. Very impressive profile. I wrote him a message, and he replied like, sure, send your CV, I can give feedback... Then he wrote back, very short, like, your profile is weak, you need a university degree, and HR admin is not real HR. It was harsh... I cried a bit. Not like, dramatic, but I felt ashamed. Because it confirmed my fear. And the tone felt judging. He did not ask questions. He did not understand that I am already working full-time. He just compared me to a standard path." (Noura)

Once ruptured, trust proved difficult or impossible to repair. Tobias described receiving a useful framework from the coach but being unable to use it: "He gave a framework that is actually not stupid... your value proposition in three parts... If someone I trust would say it, I would do it. But because he said it after invalidating me and labeling me, I did not want to engage. It felt like, if I follow his framework, I accept his worldview."

### 3.1.3 Trust in AI Tools: Safety Through Absence

Trust in AI coaching operated through a fundamentally different mechanism. Where human trust required the active presence of certain qualities (competence demonstrated, confidentiality stated, personalization shown), AI trust derived primarily from absence—the absence of judgment, ego, fatigue, and social consequence.

**The Judgment-Free Zone.** Participants consistently described AI as providing a space where they could be "messy," admit fears, and ask questions they might be embarrassed to ask humans. Amir captured this dynamic:

> "It does not require me to perform confidence. With humans, I often perform. With AI, I can write, I am scared, and it is fine." (Amir)

The freedom from performance pressure was especially salient for participants from backgrounds where appearing capable carried particular weight. Amir continued: "I come from a culture where you have pride, you do not show weakness too much. And also as an immigrant, you always want to appear capable. The AI is like, okay, you can be messy here."

Noura described using AI for emotional processing, particularly during high-stress moments:

> "When I am stressed, I write like, I feel overwhelmed, I feel stupid, I feel like I am failing. And it responds in a calm way. It asks me questions. It is non-judgmental. That is the key. I can say anything without worrying about social acceptance. If I tell my friend, I feel stupid, they might say, no you are not, stop. But with AI, I can be messy." (Noura)

She provided a specific example of this process:

> "I opened AI chat and wrote everything. Like, I am overwhelmed, I make mistakes, people think I am incompetent, I want to quit. And it responded like, it reflected my feelings, and it said, this is a stressful environment, mistakes happen, and it asked me to separate facts from interpretations... I felt relief. Like, I could breathe. It made me calmer." (Noura)

This finding suggests that AI coaching may offer particular value for populations who face additional barriers to vulnerability in human interactions—a point with implications for service design.

**Bounded Safety: Container Without Companionship.** However, participants consistently recognized this safety as bounded. The psychological shelter AI provided was described as "shallow comfort" that could contain but not truly connect. Daniel articulated this distinction: "It's comforting in a shallow way because it doesn't judge me. But it doesn't replace a friend."

Amir offered a particularly evocative metaphor: "It is like, not companionship, but containment. Like a container for thoughts." The distinction between containment and companionship marks a fundamental boundary of what AI coaching can provide—it can hold without judging, but it cannot accompany in the relational sense.

Slava explained his reluctance to use AI for emotional processing:

> "I do not like to tell a chat my feelings. It feels, I do not know, sterile... Long walks give me something AI cannot. Like, I walk and I feel my body. I breathe. I see water. I hear birds... AI cannot walk with me." (Slava)

Tobias, whose trust in human coaching had been severely damaged, still appreciated AI's bounded safety: "Emotionally, it cannot shame me. That is important. Like, AI does not say, you are too sensitive." For participants who had experienced human judgment or dismissal, AI's inability to shame represented genuine value, even if the safety it provided was limited in depth.

### 3.1.4 AI Trust Barriers: Overconfidence, Hallucinations, and Privacy

Despite AI's psychological safety, participants articulated significant trust barriers that limited their engagement. Three emerged most prominently: AI's tendency toward overconfidence, factual unreliability (hallucinations), and privacy concerns.

**Overconfidence and Hallucinations.** Participants expressed discomfort with AI presenting uncertain information with high confidence. Pierre noted: "I do not trust its taste. Like in design, taste matters. AI suggestions can be same-y, like average. And I do not trust it for high stakes decisions, because it does not know the real constraints, and it can be confidently wrong."

Several participants described specific experiences with AI hallucinations that damaged their trust. Laura #1 noted that AI once provided incorrect information about a German program: "It told me something about a German program that was wrong. And that reduced trust. But I adapted by verifying."

Alina described a similar experience with regulatory interpretation:

> "It failed when I asked about interpretation of a regulation. Like, I asked about a specific requirement, and it answered very confidently, like, you must do X. And I thought, okay, but is this true? I checked the source, and it was more nuanced. It was not exactly wrong, but it oversimplified. And in compliance, oversimplification is dangerous. Also, hallucination risk. Sometimes it invents an article number or a reference." (Alina)

Mihir recounted discovering AI inaccuracies about immigration requirements:

> "One time I asked about Blue Card requirements. It gave salary threshold numbers, but they were inconsistent. Like one message said one number, then later another. Then I checked other sources and it was confusing. So I realized I cannot rely on it for legal stuff. I can use it to understand concept, but not exact." (Mihir)

**Privacy and Self-Censoring.** Despite AI's judgment-free quality, privacy concerns led many participants to self-censor. Amir described abstracting sensitive details: "Sometimes I write something and then I think, okay, where does this go... So I sometimes abstract. I do not write exact company name."

Alina's compliance background made her particularly cautious:

> "Banking compliance is very sensitive. You cannot put client info, you cannot put internal cases. Even internal policies, sometimes you should not share externally. So I never paste confidential details. I anonymise. I write like, imagine a bank has scenario X, what is the general principle." (Alina)

This self-censoring represents a significant limitation on AI's potential value: participants who withhold contextually important information receive less tailored guidance, potentially undermining effectiveness even when trust in AI's safety is present.

### 3.1.5 Comparative Trust: Two Distinct Architectures

When asked to compare trust across modalities, participants revealed that they were not simply placing AI and human coaching on a single trust continuum but rather evaluating them against different criteria.

Mehmet captured the core distinction:

> "For human, trust is necessary. If I do not trust the coach, I will not tell the real issues... So trust is like, access to truth. With AI, trust is about reliability. Like, can I rely on the output. It is different trust." (Mehmet)

Table 3.1 summarizes the distinct trust architectures that emerged from the analysis:

| Dimension | Human Trust | AI Trust |
|-----------|-------------|----------|
| **Basis** | Presence (of competence, empathy, attention) | Absence (of judgment, fatigue, ego) |
| **Formation** | Gradual, through consistent behavior | Baseline, unless actively disrupted |
| **Destruction** | Rapid, often permanent | Less fragile, but privacy concerns can limit |
| **Type** | Relational—requires being "seen" | Functional—tool-based, task-specific |
| **Verification** | Track record, reputation, in-session behavior | Output checking, source validation |
| **Depth Potential** | High (if intact) | Bounded by lack of genuine connection |

Laura #1 exemplified the low-human-trust, high-AI-trust pattern, rating human coaching trust at 2/10 while rating AI trust at 7/10. She explained: "For private career coaches, maybe 1. For something like university career services, maybe 3... I associate it with a market of vague advice and sales tactics." Her distrust of the coaching industry as a whole drove her toward AI as a more neutral alternative.

Conversely, Lisa rated human trust at 8/10 and AI trust at 5/10, explaining: "I believe a good human coach can be trustworthy because of empathy, confidentiality, and experience... [AI] can hallucinate, like assume things. So it is mid." Her positive coaching experience anchored high human trust, while AI's limitations kept that trust moderate.

Noura offered an interesting inversion:

> "Overall I trust humans more for high-stakes decisions, but I trust AI more for private messy thoughts. Because humans can judge." (Noura)

The micro-poll ratings across all 19 participants revealed mean human trust of 6.3/10 (range 2-8) and mean AI trust of 5.7/10 (range 4-8). Notably, 12 participants (63%) rated human trust higher than AI trust, while 6 participants (32%) rated AI trust higher—the latter group often including those with negative human coaching experiences (like Tobias) or intensive AI usage patterns (like Amir and Mehmet).

---

## 3.2 Perceived Effectiveness of Human versus AI Coaching (RQ2)

The second research question asked: *How do these individuals describe and compare the perceived effectiveness of AI-based career coaching tools and human career coaching consultations for their career-related needs?* The analysis revealed that effectiveness, like trust, was not evaluated globally but rather in relation to specific task types, with participants demonstrating clear mental models of what each modality does well and poorly.

### 3.2.1 Effectiveness of Human Coaching

Human coaching was perceived as particularly effective for tasks requiring contextual knowledge, emotional support, accountability, and high-stakes decision-making.

**Contextual and Local Knowledge.** Participants repeatedly noted that human coaches could access knowledge AI simply could not provide—local market conditions, industry-specific norms, organizational politics, and cultural nuances. Tobias articulated this clearly: "Human, definitely. Someone inside or with deep industry knowledge... AI can give general ideas, but it does not know the informal parts."

This limitation was especially salient for participants navigating specific professional contexts. Pierre, a product designer in Stuttgart, noted: "The cultural nuance. Like in Stuttgart, startups are often connected to corporates. So it is not like Berlin. Also the design nuance, like in hardware, you cannot just iterate weekly."

Mehmet emphasized the importance of industry understanding:

> "A human helped because he understood my industry. Like when I met the coach from a consulting firm, he knew exactly what words to use in applications. AI gives generic templates. Human gives, in automotive, you say this." (Mehmet)

Oleg described what made his mentor effective: "Practical stuff. Like, someone telling me, in this company, you need to do this to get promoted. Or, talk to this person. Or, write your internal profile like this."

**Emotional Support and High-Stakes Decisions.** Participants consistently identified human coaching as superior for emotionally laden decisions. Silvia captured a distinction that appeared across multiple interviews: "AI for execution and structure, humans for meaning and courage... decisions need courage. And courage comes from feeling supported, not from a perfect spreadsheet."

The phrase "courage comes from feeling supported" points to a form of effectiveness that AI cannot readily provide—the emboldening that comes from human presence and belief. Participants facing high-stakes decisions (salary negotiations, quitting jobs, confronting managers) consistently preferred human support for these moments.

Slava articulated why human support was essential for his career transition question:

> "Human. 100 percent. AI can give pros and cons, but it cannot feel my situation. It cannot challenge my avoidance. It cannot hold me accountable... Because I need empathy plus challenge. AI can suggest time management tips, but it will not handle the messy reality, like my boss calling, or works council conflict, or guilt." (Slava)

**Accountability Through Relationship.** A particularly striking finding was that participants did not feel accountable to AI in the way they felt accountable to humans. Katharina stated this explicitly: "AI can generate templates and reminders, but human accountability was what made me actually do it. I do not feel accountable to AI."

Sofia explained the psychological mechanism:

> "If I say, I will talk to my manager next week, the human asks me, did you do it, what happened, what do you need now. AI can remind me, but it does not feel the same... Because I do not want to disappoint a real person. AI does not care." (Sofia)

This inability to create commitment through relationship represents a fundamental limitation on AI's effectiveness for behavior change. Participants could ignore AI reminders without social consequence; human relationships carried weight that algorithms could not replicate.

**When Human Coaching Excelled: Case Examples.** Lisa provided the highest human effectiveness rating (9/10), explaining: "Because it actually moves me forward. I have actions, I have interviews, I have confidence. The only reason it is not 10 is because it is expensive, and also because not every session is a breakthrough."

Katharina (8/10) described her coaching experience as producing concrete behavior change: "It created concrete outcomes and behaviour change, but it did not solve everything. I still have workload constraints... Coaching helped me act and reduce avoidance, but it is not magic."

Slava recalled the lasting impact of his early mentor:

> "A lot [changed]. I got moved to a better role. I became shift manager earlier than I would. And I also got confidence. Like, I started to feel I belong. And also I learned to think bigger, not only day to day." (Slava)

### 3.2.2 Effectiveness of AI Tools

AI tools were perceived as particularly effective for cognitive and structural tasks: overcoming paralysis, generating options, iterating drafts, and providing rapid breadth.

**Overcoming Blank-Page Paralysis.** Multiple participants cited AI's value in getting started. Simon explained: "It reduced stress, because the blank page problem is real. I could look at something and react. I felt, okay, I can work with this." The ability to generate an initial draft—even an imperfect one—reduced perfectionism paralysis and enabled forward movement.

Mihir described using AI to decode German job advertisements:

> "I copied the job ad into ChatGPT and I asked, explain this in English and tell me what are must-have and nice-to-have. And it gave me a structured breakdown. That was very helpful... Because it made it clear. Like, okay, I do not need to be perfect German. At least for working student, maybe English okay." (Mihir)

**Iteration Without Social Cost.** AI's tirelessness was valued for revision-heavy tasks. Participants could iterate freely without the social cost of repeated asks: "I can ask it to rewrite ten times and it doesn't get annoyed." This removed friction from the revision process, enabling more refinement than might occur with human editors whose patience could be exhausted.

Sofia described building AI tools for her own productivity:

> "I built a reporting narrative generator. So, every Monday I have to send a summary of campaign performance. I created a template where the agent reads the metrics from a CSV and writes a first draft narrative, like, this week ROAS increased due to X, but CAC rose in Y segment, recommended action is Z. It is not perfect, but it gives me a starting point." (Sofia)

**Structuring Chaos.** Laura #2 provided the highest AI effectiveness rating (9/10), attributing her major career decision largely to AI-supported clarity: "AI is very good. Because it can turn chaos into steps. And then humans can validate the steps. So AI creates the plan, human makes it realistic."

Her three-week AI-intensive process involved frameworks, criteria, scenarios, and ultimately a decision memo. She described a pivotal moment: "It asked me... what are you protecting by not deciding. And I wrote, I am protecting my identity as someone who made the safe choice and the prestigious choice... And when I saw it in text, it was like, okay, I cannot unsee it. That was a real coaching moment."

Noura described a similar structuring benefit:

> "I asked AI: I have 6 hours per week, I want to learn SQL and Power BI, I have HR background, make me a plan. It gave me weekly topics: week one, select statements, filtering, sorting. Week two, group by and aggregates. Week three, joins. Week four, subqueries. Then it suggested small HR examples, like, employee table, absence table. That made it feel relevant." (Noura)

**Speed and Availability.** AI's constant availability was valued, particularly for late-night processing or moments when human support was inaccessible. Amir noted: "It is accessible when humans aren't—at 1am, on weekends, during crises. This constant availability is itself a form of safety: there's always a container available."

Mehmet emphasized availability as a key advantage:

> "AI is always available. Like, at 11pm when I have a thought, I can write it. Human mentor, no. Also AI does not get tired of my questions. I can ask, explain again, and it does." (Mehmet)

**AI Limitations in Effectiveness.** However, participants identified clear boundaries to AI effectiveness. Beyond the trust barriers discussed earlier (hallucinations, overconfidence), AI was seen as limited in several effectiveness-relevant ways:

Jonas provided the lowest AI effectiveness rating for career questions (3/10), while rating AI effectiveness for technical work at 8/10. He explained: "For career situation, like leadership path, maybe 3. For technical growth, it is like 8." This split rating illuminates that AI effectiveness is highly domain-specific; the same participant can find AI extremely effective for one task type and ineffective for another.

Mihir described a moment when AI increased rather than reduced anxiety:

> "One night, I was scrolling LinkedIn. I saw someone from my university, same age, got a full-time role at a big company. And then I felt like, I am failing. So I opened AI... It calmed me, but not grounded. Then I asked, okay, give me a plan. And it gave a plan, but it was too optimistic. Like, apply to 50 jobs per week, build two projects, learn German to B2 in 2 months. And then I felt worse because it was unrealistic. So sometimes AI increases anxiety." (Mihir)

### 3.2.3 Comparative Perceptions: Complementarity Over Competition

A central finding was that participants did not view AI and human coaching as competing for the same functions but as complementary, with each excelling in distinct domains. Rather than asking "which is more effective?", participants asked "for which tasks is each more effective?"

Alina articulated the complementary logic:

> "AI is good for brainstorming options. Like, list roles in ESG in banking, list skills. It can give a broad map. But I would prefer a human to refine it for Germany and for my profile. Because AI can list everything, but it does not tell you what is realistic." (Alina)

Table 3.2 presents a summary of perceived strengths derived from participant interviews:

**Table 3.2: Perceived Strengths of AI-Based versus Human Coaching**

| Task Domain | Preferred Modality | Representative Quote |
|-------------|-------------------|---------------------|
| Generating initial options | AI | "It can list many roles and explain" (Simon) |
| Translating/decoding job ads | AI | "Explain this in English and tell me what are must-have and nice-to-have" (Mihir) |
| Iterating drafts | AI | "I can ask it to rewrite ten times and it doesn't get annoyed" |
| Structuring chaos | AI | "It can turn chaos into steps" (Laura #2) |
| Overcoming paralysis | AI | "The blank page problem is real" (Simon) |
| Learning plans and skill-building | AI | "It gave me weekly topics... that made it feel relevant" (Noura) |
| Local market knowledge | Human | "In automotive, you say this" (Mehmet) |
| Office politics navigation | Human | "It does not know the informal parts" (Tobias) |
| Accountability | Human | "I do not feel accountable to AI" (Katharina) |
| High-stakes decisions | Human | "Courage comes from feeling supported" (Silvia) |
| Emotional processing | Human | "AI cannot hold my fear" |
| Values clarification | Human | "AI can list pros and cons, but it cannot ask the right uncomfortable question" (Katharina) |
| Interview practice (delivery) | Human | "Human can tell if I sound like operations guy who is hiding insecurity" (Slava) |
| German language validation | Human | "A human with market knowledge is needed to tell me what recruiters want" (Alina) |

The micro-poll ratings across participants revealed mean human effectiveness of 5.6/10 (range 2-9) and mean AI effectiveness of 6.5/10 (range 3-9). Notably, AI effectiveness ratings showed lower variance than human effectiveness ratings (standard deviation 1.5 vs 1.8), suggesting that AI provided more consistent (if bounded) value, while human effectiveness was more variable—exceptional when the match was right, but poor when it wasn't.

Alina captured this paradox in her own ratings:

> "So you have higher effectiveness for AI than for the human coaching you experienced, but lower trust... Yes, exactly. That is the paradox." (Alina)

Twelve participants (63%) rated AI effectiveness higher than human effectiveness, often because AI provided reliable value for their daily needs while they lacked access to high-quality human coaching. Conversely, five participants (26%) rated human effectiveness higher, typically those with positive coaching experiences (Lisa, Katharina, Pierre).

---

## 3.3 Interplay of Trust and Perceived Effectiveness, and Influence on Coaching Preferences (RQ3)

The third research question asked: *In participants' narratives, how are trust and perceived effectiveness connected, and how do they influence preferences for AI, human, or hybrid coaching models?* The analysis revealed a causal mechanism linking trust to effectiveness, important exceptions to this mechanism, and a near-universal preference for hybrid models that leverage the distinct strengths of each modality.

### 3.3.1 Trust as Gateway: The Causal Mechanism

The most significant finding regarding the trust-effectiveness relationship was that trust functions as a *gateway mechanism* controlling the depth of disclosure, which in turn determines advice quality and willingness to act. This mechanism was articulated explicitly by multiple participants.

Katharina stated: "For me, trust is the gateway. If I trust the coach, I admit the real thing I am avoiding... Without trust, I stay on the surface."

Alina articulated the same principle:

> "Trust is like the gate. If I do not trust the coach, I will not share the real constraints. Like my fear about residency, or my financial obligations, or my insecurity about German. And then the coach can only give generic advice. So trust enables depth. And depth makes it effective." (Alina)

Sofia elaborated on trust as permission:

> "Trust is like, um, permission. If I trust a mentor, I tell the real story. Like, I say, I am scared, I feel stupid, I do not understand the meeting dynamics. And then they can help. If I do not trust, I only talk about safe topics, like CV. Then it is less effective." (Sofia)

Noura expressed the same mechanism:

> "Trust is like, the door. If I trust, I share the real problem. Like I can say, I feel not credible, I am afraid. If I do not trust, I will just ask surface questions, like, what course should I do. Then the advice is surface. So trust enables depth, and depth makes it effective." (Noura)

Tobias elaborated on the consequences of low trust: "When I distrust the person, I do not share real information. I share a safe version. Then the advice is based on wrong or incomplete input." This creates what Amir called "the performance problem": "With humans, if I do not trust, I will not share real fears. I will perform. Then the coach will coach the performance, not the reality."

Slava expressed this most vividly:

> "Trust is like entry ticket. If I do not trust, I will not say the real thing. I will say the safe version. Like, I am busy. Not like, I am scared, or I feel trapped, or I fantasize about leaving because I hate this. If I do not trust, then the coach works on surface. When I trusted my mentor early on, I told him, I feel like outsider, I feel angry, I feel insecure. And then he could help. So trust enables depth. Depth enables effectiveness." (Slava)

The mechanism can be represented as a causal chain:

**Trust → Disclosure Depth → Advice Quality → Action Willingness → Effectiveness**

When trust is present, participants share authentic concerns, enabling coaches to address actual blockers. When trust is absent, participants share surface-level or performed versions of their situations, and coaches address issues that may not be the real barriers—producing advice that fails to move participants forward.

Mihir described how trust affects action:

> "Trust is like, it decides how honest I can be. If I do not trust, I will only show the nice story. Like, I will say, I am fine, I just want CV tips. But the real issue is, I am scared and confused. If I trust, I can say, I feel behind, I do not know my direction, I am panicking. Then the person can help with the real problem. Also, if I trust their competence, I will follow their advice. If I doubt, I will not act. So trust affects action." (Mihir)

Daniel described trust as amplifying impact: "Trust is like a multiplier." The same suggestion lands differently depending on the source's trustworthiness. Tobias provided a vivid illustration: "If someone I trust would say it, I would do it. But because he said it after invalidating me and labeling me, I did not want to engage. It felt like, if I follow his framework, I accept his worldview."

This finding has significant implications: coaching effectiveness is not purely a function of coach competence but depends critically on the trust conditions that enable full engagement.

### 3.3.2 When Trust and Effectiveness Diverge

Despite the gateway mechanism, trust and effectiveness could diverge in both directions—a finding that adds important nuance to the relationship.

**Trusted but Not Effective.** Several participants described sources they trusted deeply but found ineffective for practical guidance. Daniel explained: "I trusted her as a person. But she didn't give concrete actions. I left feeling warm, but nothing changed."

Laura #2 described a similar pattern with a close friend: "My friend in Barcelona. I called her crying... she was very supportive... I trusted her emotionally. It felt very good. But after the call, I still had no plan. So it was trusted but not effective."

Sofia described her first mentor in these terms:

> "Trusted, but not effective because no continuity. I still think she is great, but it did not help my career problem in the end... It was motivating. After the first call I felt, like, I can do it. She gave me some frameworks... But after maybe two calls, it became messy. She started cancelling... It broke the trust in reliability. Like I still liked her as a person... But it was not effective anymore because there was no follow through." (Sofia)

Noura identified the pattern with a supportive but vague contact:

> "There was a woman, she was super kind. She wrote long messages like, you are doing great, you are strong, I believe in you. Very sweet. But when I asked, okay, how do I move into people analytics, she said, just follow your passion, apply and it will happen. That was trusted in a way, because she was safe and kind. But it was not effective. I ended the chat feeling warm, but still lost." (Noura)

These cases reveal that trust is necessary but not sufficient for effectiveness; structure and actionable guidance are also required.

**Effective but Not Trusted.** Conversely, participants described deriving value from sources they did not fully trust. Daniel noted: "His advice had useful parts... But I didn't trust him... I still took some tips, but it cost energy. If you don't trust, you second-guess everything, so it's harder to act."

Amir described AI in similar terms: "It can be effective in giving me a language to think, but I do not trust it because it might be biased toward certain narratives." He used AI's outputs while maintaining skepticism about its underlying assumptions.

Sofia described her second mentor as effective despite low emotional trust:

> "She [second mentor] was German, very direct, very structured. She was like, okay, what is your goal, what is your plan, send me your CV, we will review... effective, because she pushed me. But I did not trust her emotionally... She gave concrete tasks. She told me to write a stakeholder update email in a certain format. And it worked. My manager liked it. So effective. But I did not feel warm. I did not feel safe. So I did not share the deeper anxiety." (Sofia)

Silvia articulated the distinction between trust as gate versus trust as filter: "With AI, I use it because it is effective for tasks, even if trust is partial. With humans, if trust is not there, I do not use it, so effectiveness cannot happen. So trust is like a gate for humans. For AI, trust is like a filter, not a gate."

Alina expressed the same insight:

> "I use AI even when I do not fully trust it. Like, I know it can be wrong, but it is still effective for drafting. So that is, how you said, effective but not fully trusted. I trust it as a tool, not as an authority. I trust it to generate text options, not to decide." (Alina)

This insight illuminates why AI can achieve effectiveness with partial trust: for bounded tasks that don't require emotional vulnerability, participants can extract value through verification rather than faith. The cognitive cost is higher—more second-guessing, more checking—but utility is still accessible.

### 3.3.3 Task Type as Moderator

The relationship between trust and effectiveness was moderated by task type. For emotional, identity-related, or high-stakes tasks, trust was essential—the gateway mechanism dominated. For bounded cognitive tasks, trust was helpful but not essential—effectiveness could occur through verification.

Noura articulated the moderation clearly:

> "With AI, if I do not trust accuracy, I do not follow the advice. I might still use it as brainstorming. Like, it can be effective as a mirror, even if I do not trust it as truth. But if it makes a wrong assumption, it can confuse me. So trust in AI is more like, trust in reliability and boundaries. If I know it is good for structuring, then it is effective for that. If I mistakenly trust it for legal topics, it can be harmful. So I keep it in its lane." (Noura)

Figure 3.1 illustrates this moderation:

```
TASK TYPE MODERATION OF TRUST-EFFECTIVENESS LINK

                    TRUST REQUIREMENT
                    Low ◄────────────────► High
                         │
    COGNITIVE/BOUNDED    │    EMOTIONAL/IDENTITY
    ─────────────────    │    ─────────────────────
    • CV drafting        │    • Values clarification
    • Option generation  │    • Processing rejection
    • Interview prep     │    • Salary negotiation
    • Scheduling         │    • Career pivots
    • Learning plans     │    • Confidence building
                         │
    AI can be effective  │    Trust essential for
    with verification    │    authentic engagement
                         │
    [Theme 5 applies]    │    [Theme 4 applies]
```

This moderation explains the quantitative finding that 58% of participants rated AI effectiveness higher than AI trust: for bounded tasks, effectiveness could occur even with limited trust through verification and pragmatic use.

### 3.3.4 Influence on Preferences: The Hybrid Ideal

Given the complementary strengths of AI and human coaching, and the different trust-effectiveness dynamics for each, how did these perceptions shape participants' preferences?

**Universal Hybrid Preference.** The most striking finding was that all 19 participants expressed preference for some form of hybrid model rather than AI-only or human-only support. This preference was not merely additive ("both are good") but sequential and strategic—participants had clear mental models of when to use each modality.

Laura #2 provided the most detailed articulation of a hybrid workflow:

> "Week one, AI helps you map options, values, criteria. Then you meet a human coach... Then AI helps you build scenarios... Then a human helps you reality-check... Then AI helps you with execution." (Laura #2)

This sequencing—AI for preparation, human for pivotal moments, AI for follow-up—appeared across multiple interviews in varying forms.

Mehmet described his preferred division of labor:

> "AI for preparation, drafts, language. Human for big decisions, for accountability, and for industry reality check... The ideal is, AI does the work, human does the judgment." (Mehmet)

Noura outlined a detailed hybrid workflow:

> "I would start with AI onboarding, like I explain my situation, my constraints, Ausbildung, part-time studies, Düsseldorf, HR coordinator, goals. Then AI suggests a few paths and learning plan. Then it suggests three mentors that fit. I choose one. Then I have a short call with the mentor, like 30 minutes, to validate plan and set goals. Then AI supports me weekly with tasks. Then every month, a human check-in, like, adjust and reflect. That would be perfect." (Noura)

**AI for Preparation, Humans for Pivots.** A consistent theme was using AI to prepare for human conversations, making expensive human time more productive. Amir described this dynamic:

> "I used AI to clarify what I want to ask... The AI helped me generate a structured agenda. Then in the meeting, I was clear. She responded better. So the AI indirectly improved a human interaction." (Amir)

Laura #2 explained the reasoning: "Because humans get tired if you are messy. Like, you call a friend and you are like, I do not know... But if you say, I have three paths, here is what I value... then the conversation is productive."

Sofia articulated her ideal division:

> "AI: brainstorming options, structuring my thoughts, drafting messages, CV versions, interview prep questions, meeting prep checklist, stakeholder FAQ, like all that. Human: role strategy, reading company politics, helping me with confidence and presentation delivery, and accountability, like, did you actually do the thing." (Sofia)

This finding—that AI preparation enables better human conversations—suggests synergies beyond simple task division: AI's structuring capacity can enhance the quality of subsequent human engagement.

**Sequencing Depends on Starting State.** Not all participants would start with AI. Katharina noted that she needed human safety first: "At the beginning, I needed relational safety to admit what I was avoiding. AI could not give me that. Once I had momentum, AI could support maintaining it."

This suggests that optimal sequencing depends on the participant's current state: those needing to overcome avoidance or establish safety may require human contact first, while those with clarity but execution challenges can productively begin with AI.

**Cost and Access Considerations.** Practical considerations also drove hybrid preferences. Human coaching is expensive; AI can extend access and fill gaps. Daniel outlined his ideal: "AI first... Then human: 45 minutes to decide my positioning... and maybe a short follow-up." This configuration concentrates expensive human time on high-value decisions while using AI for preparation and follow-up.

Lena expressed similar logic: "I do not want to pay a human for everything. Like, if I have to pay 100 euros per hour for someone to rewrite my bullet points, it feels wasteful. But I would pay for someone to help me with strategy and confidence."

Mihir captured the economic logic:

> "That sounds like exactly what I need. Because AI can do the fast work, and human can do the deeper work. Also it could reduce cost, because human time is expensive. So if AI does preparation, human session is more focused." (Mihir)

**Trust Requirements for Hybrid Models.** When asked what would make them trust a hybrid platform, participants identified several requirements:

1. **Clear boundaries**: What AI does versus what humans do must be transparent
2. **Data protection**: How information is shared between modalities
3. **Quality control**: Standards for human elements
4. **No deception**: AI should not pretend to be human or claim capabilities it lacks

Katharina articulated the concern: "If AI is positioned as advice, that is risky. If it is positioned as structure and drafting support, okay. And the human needs to own the ethical part and the nuance."

Alina specified her requirements:

> "Clear boundaries. Like, what data goes to AI, what stays private. Maybe an option to keep everything local, but that is technical. Also transparency: who are the human experts, what is their background. And a process. Like, step 1 AI intake, step 2 human session, step 3 AI follow-up tasks, step 4 check-in. That would make me feel safe. And also confidentiality policies, like GDPR, data handling. In Germany, people care." (Alina)

Slava expressed conditional openness:

> "That sounds ideal, if the human part is ethical and adaptive... But it could feel creepy if it is not transparent. Like if it collects too much. If it stores sensitive personal notes. Also if it is not transparent. Like, who sees what. If the human mentor can see my AI conversations, I need control. Because sometimes I might write things I do not want to share." (Slava)

### 3.3.5 Preference Subgroups

While hybrid preference was universal, nuances emerged in how participants weighted the components:

**Human-Primary Preference.** Participants like Lisa, Pierre, and Katharina—those with positive human coaching experiences—expressed preferences that weighted human coaching heavily, with AI in a supporting role. Lisa rated human effectiveness at 9/10 and AI at 6/10, reflecting her experience that human coaching "actually moves me forward."

**AI-Primary Preference.** A smaller group, including Laura #1 and Amir, expressed preferences that weighted AI more heavily, with humans for specific moments. Laura #1's extreme distrust of the coaching industry (human trust: 2/10) combined with high AI effectiveness (8/10) produced a pattern where AI was primary and human support was minimally sought.

Amir, despite trusting AI highly (8/10), still valued human input for validation: "AI can help me structure my thoughts, but I still want a human to say, yes, that makes sense, or to catch blind spots I share with the AI."

Noura indicated AI as her current primary choice due to access constraints:

> "If I had to choose only one, only AI or only human, which would you pick right now? Right now, only AI. Because I actually have access and it helps me. Human is too uncertain. But ideally, hybrid." (Noura)

**Balanced Hybrid Preference.** Most participants fell between these poles, valuing both modalities for different purposes without strongly weighting one over the other. Their preferences were task-driven rather than modality-driven: AI for execution and structure, humans for meaning and courage, as Silvia articulated.

**Context-Dependent Variation.** Several participants noted that their preferences would shift based on circumstances. If cost were not a barrier, many would use more human coaching. If AI capabilities improved (better accuracy, verified information), trust and usage would increase. The current hybrid preference reflects both genuine complementarity and practical constraints.

---

## 3.4 Summary of Findings

The thematic analysis of 19 interviews with early-career individuals in Germany reveals several key findings regarding trust and effectiveness in AI-based versus human career coaching.

**Regarding RQ1 (Trust):** Participants described trust in human coaching as a multi-component architecture requiring demonstrated competence, explicit confidentiality, personalization ("being seen"), and the capacity to challenge without shaming. This architecture builds gradually through consistent behavior but can collapse instantly through a single act of dismissiveness or invalidation, with trust rupture often proving permanent. In contrast, trust in AI derives primarily from absence—the absence of judgment, ego, fatigue, and social memory—creating a "judgment-free zone" particularly valued by immigrants and those from cultures emphasizing capability. However, AI trust is bounded by concerns about overconfidence, hallucinations, and privacy, leading to self-censoring that limits disclosure depth. Quantitatively, mean human trust (6.3/10) slightly exceeded mean AI trust (5.7/10), though individual variation was substantial.

**Regarding RQ2 (Effectiveness):** Participants perceived human coaching as more effective for tasks requiring contextual knowledge, emotional support, accountability, and high-stakes decision-making. Human coaches could access local market conditions, navigate office politics, and provide the "courage" that comes from feeling supported. AI was perceived as more effective for cognitive and structural tasks: overcoming blank-page paralysis, generating options, iterating drafts, translating job advertisements, building learning plans, and structuring chaos into actionable steps. Rather than viewing the modalities as competing, participants viewed them as complementary, matching each to appropriate tasks. Quantitatively, mean AI effectiveness (6.5/10) slightly exceeded mean human effectiveness (5.6/10), though human effectiveness showed higher variance—exceptional when the match was right, but poor when it wasn't.

**Regarding RQ3 (Trust-Effectiveness Connection and Preferences):** Trust and effectiveness were linked through a gateway mechanism: trust enables disclosure depth, which determines advice quality, which affects action willingness. Participants described this variously as trust being "like the gate" (Alina), "like permission" (Sofia), "like the door" (Noura), and "like entry ticket" (Slava). However, this mechanism was moderated by task type—essential for emotional/identity work, less critical for bounded cognitive tasks where verification can substitute for trust. The mechanism also admitted exceptions: trusted sources sometimes lacked structure (trusted but not effective), while distrusted sources could still provide useful outputs at cognitive cost (effective but not trusted). These dynamics produced a universal preference for hybrid models that sequence AI and human support strategically: AI for preparation and follow-up, humans for pivotal moments. A particularly significant finding was that AI preparation enables better human conversations by reducing "messiness" and focusing expensive human time on high-value decisions.

Table 3.3 summarizes the quantitative ratings across participants:

**Table 3.3: Summary of Participant Ratings (0-10 scale)**

| Measure | Human Trust | AI Trust | Human Effectiveness | AI Effectiveness |
|---------|-------------|----------|---------------------|------------------|
| Mean | 6.3 | 5.7 | 5.6 | 6.5 |
| Median | 7 | 6 | 6 | 6.5 |
| Range | 2-8 | 4-8 | 2-9 | 3-9 |
| Std Dev | 1.4 | 1.3 | 1.8 | 1.5 |

**Table 3.4: Individual Participant Citations Overview**

| Participant | Key Contribution to Findings |
|-------------|------------------------------|
| Amir | Judgment-free zone, performance problem, AI for preparation |
| Simon | Competence criteria, blank-page paralysis |
| Katharina | Challenge without shame, accountability, trust as gateway |
| Jonas | Confidentiality requirements, AI domain-specificity |
| Tobias | Trust rupture, somatic response, same message different delivery |
| Daniel | Container vs companionship, trust as multiplier |
| Pierre | Overconfidence concerns, cultural nuance, design taste |
| Laura #2 | Chaos to steps, pivotal AI moment, hybrid workflow |
| Silvia | Courage from support, trust as gate vs filter |
| Lisa | High human trust, effectiveness ratings |
| Laura #1 | Low human trust, coaching industry distrust |
| Lena | Cost considerations, strategic human use |
| Oleg | Mentor competence, procedural knowledge, protection |
| Mehmet | Trust as access to truth, AI availability, industry knowledge |
| Sofia | Trust as permission, accountability mechanism, mentor reliability |
| Alina | Trust as gate, effectiveness paradox, hybrid requirements |
| Noura | Trust enabling depth, AI journaling, hybrid workflow |
| Slava | Trust as entry ticket, burnout context, challenge balance |
| Mihir | Trust affecting action, AI anxiety, economic logic |

These findings suggest that early-career individuals in Germany hold nuanced, task-specific views of AI and human coaching rather than wholesale endorsement or rejection of either modality. They intuitively understand the distinct trust architectures and effectiveness profiles of each, and they design (or would design) hybrid workflows that leverage complementary strengths. For organizations considering hybrid coaching models, these findings suggest that success depends not merely on combining AI and human elements but on thoughtful sequencing, clear boundaries, and transparency about what each component provides.

The next chapter will discuss these findings in relation to existing literature, consider their implications for practice, and address limitations and directions for future research.

---

*Note: All participant names are pseudonyms. Line numbers reference the Master Interview Transcripts in Appendix A.*
