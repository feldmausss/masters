# Chapter 5: Conclusion

## 5.1 Summary of the Research

This thesis set out to explore how early-career individuals in Germany perceive and compare trust and effectiveness in AI-based career coaching tools versus human career coaching consultations. In a context where artificial intelligence is increasingly positioned as a solution for scaling personalized support, understanding how potential users actually experience and evaluate these modalities becomes crucial for both theoretical understanding and practical service design.

Through semi-structured interviews with 19 early-career professionals and systematic thematic analysis, the study addressed three interconnected research questions concerning trust (RQ1), perceived effectiveness (RQ2), and the relationship between trust and effectiveness as it influences coaching preferences (RQ3). The findings reveal a landscape more nuanced than simple preferences for one modality over another, instead uncovering distinct trust architectures, complementary effectiveness profiles, and a causal mechanism linking trust to outcomes.

---

## 5.2 Key Findings and Contributions

### Trust Operates Through Distinct Mechanisms

The study found that trust in human coaching operates as a multi-component architecture requiring demonstrated competence, explicit confidentiality, personalization ("being seen"), and the capacity to challenge without shaming. This trust is relational—built gradually through consistent behavior—but notably fragile, capable of instant and often permanent collapse through a single act of dismissiveness or invalidation. Participants described trust rupture in embodied, somatic terms, revealing that relational trust operates at a pre-cognitive, whole-person level.

Trust in AI, by contrast, derives from absence—the absence of judgment, fatigue, ego, and social memory. This "judgment-free zone" provides genuine psychological safety for certain populations, particularly those facing cultural or identity-based performance pressure. Immigrants and those from cultures emphasizing pride and capability described AI as a space where they could "be messy" without social consequence. However, this safety remains bounded: participants recognized it as "shallow comfort"—containment without companionship—and self-censored due to privacy concerns, limiting disclosure depth.

These distinct trust architectures suggest that AI and human coaching are not simply different points on a single trust continuum but operate through fundamentally different relational logics.

### Effectiveness Reflects Complementary Competencies

Rather than viewing AI and human coaching as competing for the same functions, participants articulated clear mental models of complementary competencies. Human coaches were perceived as more effective for contextual knowledge, emotional support, accountability, and high-stakes decision-making—domains where relational presence provides irreplaceable value. AI was perceived as more effective for cognitive and structural tasks: overcoming blank-page paralysis, generating options, iterating drafts, and transforming chaos into actionable steps.

This complementarity finding challenges framings that position AI as either replacing or failing to match human coaches. Instead, the appropriate question becomes: for which tasks is each modality best suited? Participants naturally developed task-appropriate theories of comparative advantage and applied them in their usage patterns.

### Trust Functions as a Gateway to Effectiveness

Perhaps the most significant theoretical contribution is the identification of a gateway mechanism linking trust to effectiveness. The causal chain—Trust → Disclosure Depth → Advice Quality → Action Willingness—explains how trust produces outcomes, not merely that trust and effectiveness correlate. When trust is present, participants share authentic concerns, enabling coaches to address actual blockers. When trust is absent, participants share performed versions of their situations, and coaches address issues that may not be the real barriers.

This mechanism is moderated by task type. For emotional and identity work, trust remains essential—the gateway cannot be bypassed. For bounded cognitive tasks, effectiveness can occur with partial trust through verification and pragmatic engagement. This moderation explains why AI can be effective for certain tasks despite lower trust: participants extract value through a "verify then apply" mode rather than requiring faith.

### Universal Preference for Hybrid Models

The study found that all 19 participants preferred some form of hybrid model rather than exclusive reliance on either modality. This preference was not merely additive but sequential and strategic—participants articulated clear mental models of when to use each modality. The typical pattern involved AI for preparation and follow-up (bounded tasks with lower trust requirements) and humans for pivotal moments (high-stakes decisions requiring trust-enabled depth).

A particularly significant finding was that AI preparation enables better human conversations. When participants arrive at human sessions with pre-structured thinking, the dialogue becomes more productive. This synergy suggests that hybrid models can exceed the sum of their parts rather than merely combining two parallel services.

---

## 5.3 Practical Contributions

The study's findings translate into concrete recommendations for organizations designing hybrid AI-human coaching services. For ODA specifically, the research suggests:

1. **Design AI components for transparency and appropriate warmth**, explaining recommendations, acknowledging uncertainty, and avoiding robotic interaction patterns that signal low competence.

2. **Establish human-mediated introduction** of AI components, leveraging trust transfer from humans to systems through explicit endorsement.

3. **Implement clear role divisions** aligned with participants' mental models: AI for structure, iteration, preparation, and follow-up; humans for context, emotional processing, accountability, and pivotal decisions.

4. **Enable seamless human escalation** from AI interactions, providing the safety net that makes initial AI engagement feel lower-risk.

5. **Use AI to enhance rather than replace human sessions**, with AI-generated summaries, pre-session briefs, and between-session support that makes expensive human time more productive.

6. **Communicate the hybrid value proposition clearly**, addressing trust concerns directly and setting appropriate expectations about what each component can and cannot provide.

These recommendations are grounded in participants' expressed needs, preferences, and concerns, offering an evidence base for design decisions.

---

## 5.4 Theoretical Contributions

Beyond practical implications, the study contributes to several theoretical domains:

**Human-AI Collaboration.** The findings extend models of human-AI collaboration by documenting how users naturally develop task-appropriate mental models for dividing work between modalities. Hybrid system design may benefit from surfacing and supporting these intuitive models rather than imposing designer-defined divisions.

**Trust in Technology.** The distinction between relational trust (human) and functional trust (AI) enriches understanding of how trust operates differently across system types. The finding that AI trust derives from absence rather than presence challenges assumptions that AI should attempt to replicate human trust-building behaviors.

**Coaching Effectiveness.** The gateway mechanism provides a process-level explanation for why trust matters in coaching contexts, complementing outcome-focused research with mechanistic understanding. Trust is not merely a nice-to-have but a functional prerequisite that determines information quality in the coaching relationship.

**Accessibility and Equity.** The cultural and immigrant safety finding suggests that AI coaching may offer particular value for populations facing identity-based barriers to help-seeking—a perspective underrepresented in current literature that tends to position AI as simply less effective than human alternatives.

---

## 5.5 Limitations and Future Directions

The study's limitations point toward productive future research. The small, qualitative sample appropriate for exploration limits generalizability; larger quantitative studies could assess the prevalence of these patterns across populations. The cross-sectional design captured perceptions at a single point; longitudinal research could track how trust and effectiveness perceptions evolve with experience. Some participants had limited AI coaching experience, leading to hypothetical rather than experience-based responses; experimental research assigning participants to actual coaching conditions could validate stated preferences against lived experience.

Future research should also directly evaluate hybrid implementations once deployed. Do the design recommendations proposed here actually enhance trust and effectiveness in practice? What barriers emerge that were not anticipated? Iterative research accompanying service development would refine understanding of what works and for whom.

---

## 5.6 Final Reflections

This thesis began with a question about how early-career individuals perceive AI versus human coaching—a question motivated by the practical challenge of designing hybrid services that users will actually trust and find valuable. The answer that emerged is more nuanced than simple preferences: participants articulated sophisticated theories of what each modality does well, identified mechanisms linking trust to effectiveness, and expressed preferences for strategic combination rather than exclusive reliance on either.

The findings suggest that the future of career coaching likely lies not in AI replacing human coaches nor in AI failing to find a role, but in thoughtful integration that leverages complementary strengths. AI can provide the structure, iteration, and accessibility that make coaching resources more available; humans can provide the contextual understanding, relational accountability, and courage-enabling presence that high-stakes decisions require. The challenge for service designers is to orchestrate this integration in ways that earn user trust and deliver perceived effectiveness.

By giving voice to early-career professionals' experiences and perceptions, this study contributes to a more nuanced understanding of technology's role in career development. It offers guidance to practitioners—like ODA—seeking to integrate AI in people-centric ways, and it opens productive directions for research on trust, effectiveness, and human-AI collaboration in coaching contexts. As artificial intelligence becomes increasingly capable and prevalent, understanding how humans experience, evaluate, and integrate these systems into their lives becomes ever more important. This thesis offers one contribution to that understanding, grounded in the lived experiences of those navigating early-career challenges with whatever support—human, AI, or both—they can access.

The balance between technological innovation and the irreplaceable human touch will continue to evolve. What remains constant is the need for coaching—in whatever form—to be trusted by those it serves and effective for the goals they pursue. This study suggests that both trust and effectiveness are achievable, but they require different things from different modalities, and they are best pursued together rather than in isolation.

---

*Word count: Approximately 1,600 words*
